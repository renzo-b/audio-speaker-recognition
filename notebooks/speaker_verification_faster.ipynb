{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6e5067-2050-4ba3-a4b1-a0d9122bc44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renzo/anaconda3/envs/dl-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from src.datasets import get_dataset\n",
    "from src.visualization import (calculate_statistics, play_audio, plot_mfccs,\n",
    "                               plot_specgram, plot_waveform,\n",
    "                               visualize_random_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ea269-4993-4d42-a4b5-9e041fab18da",
   "metadata": {},
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4024e60-7357-430f-87a4-d8430e9f2802",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "LIBRISPEECH\n",
    "LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned.\n",
    "\n",
    "Section contents:\n",
    "- Visualize random sample\n",
    "- Calculate dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8fede5-1e69-493a-9726-81a848a1629a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're about to download librispeech with url train-clean-100\n",
      "\n",
      " Done downloading\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"librispeech\"\n",
    "url = 'train-clean-100'\n",
    "dataset = get_dataset(dataset_name, url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7d76b-aa2e-48ea-b90a-794d4249a39e",
   "metadata": {},
   "source": [
    "#### Visualizing data in random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c48bf2-281d-4c6c-9c1c-cb1691c02402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_random_sample(dataset, plot_wave=True, plot_spectogram=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15b323-e456-4cd8-bc33-314c9d944827",
   "metadata": {},
   "source": [
    "#### Calculate statistics in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fa38e5-1325-4499-989f-5d8e18bdcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform_lengths = []\n",
    "# sample_rates = []\n",
    "# transcript_lengths = []\n",
    "# speaker_ids = []\n",
    "# audio_durations = []\n",
    "# waveforms = []\n",
    "# transcripts = []\n",
    "\n",
    "# for i in tqdm(range(dataset.__len__())):\n",
    "#     (waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id) = dataset.__getitem__(i)\n",
    "#     speaker_ids.append(speaker_id)\n",
    "\n",
    "    # waveform_lengths.append(waveform[0].shape[0])\n",
    "    # sample_rates.append(sample_rate)\n",
    "    # transcript_lengths.append(len(transcript))\n",
    "    # waveforms.append(waveform[0].numpy())\n",
    "    # transcripts.append(transcript)\n",
    "\n",
    "    # num_channels, num_frames = waveform.shape\n",
    "    # duration = num_frames / sample_rate\n",
    "    # audio_durations.append(duration)\n",
    "\n",
    "# dataset_stats = pd.DataFrame.from_dict(\n",
    "# {\n",
    "#     \"waveform_lengths\" : waveform_lengths,\n",
    "#     \"sample_rates\" : sample_rates,\n",
    "#     \"transcript_lengths\" : transcript_lengths,\n",
    "#     \"audio_durations\" : audio_durations,\n",
    "# }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120913c7-7f1a-4ce8-bd79-561b11d6a1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_spkr = pd.DataFrame(columns=['speaker_id', 'duration'])\n",
    "# df_spkr['speaker_id'] = speaker_ids\n",
    "# # df_spkr['duration'] = audio_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a361612f-aa53-4c27-8aae-3e2d05826802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spkr.to_csv('df_speaker_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f2d8e5-0d58-4b78-9590-76ed77cbbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spkr = pd.read_csv('df_speaker_ids.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93744c44-df17-449b-939b-e8f3be7e1abf",
   "metadata": {},
   "source": [
    "Because on average the duration of each speaker is 4.8 or higher, we decided to use a window_size of 4 seconds. Samples shorter than 4 seconds will be removed from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df5bcb5-42a8-49fd-a128-4e420d36d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spkr.groupby('speaker_id').mean().sort_values(by='duration', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243e5db-e8c0-47be-9771-308c91fbfdc0",
   "metadata": {},
   "source": [
    "Number of samples for each speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "290a11a7-a729-4914-910d-3fcaadab4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spkr.groupby('speaker_id').size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54059c1d-2ca5-4faf-8e82-4e3f7609397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = calculate_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102617ee-b8f3-4dfa-aba7-9e628739de08",
   "metadata": {},
   "source": [
    "Unique speaker ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3bdd051-5a4a-4ea9-8f2c-e2b4632bbfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(df_spkr['speaker_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e13cbb-4888-4b39-b147-bd79a407cbcf",
   "metadata": {},
   "source": [
    "Visualize spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b4960-7c18-4ac9-b856-2e0fcf2e4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_speaker_waveform = pd.DataFrame(columns=['speaker_id', 'waveform', 'transcript'])\n",
    "# df_speaker_waveform['speaker_id'] = speaker_ids\n",
    "# df_speaker_waveform['waveform'] = waveforms\n",
    "# df_speaker_waveform['transcript'] = transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1460052-4def-4d19-8cff-15142743e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_speaker_waveform[df_speaker_waveform['speaker_id']==84].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747ea29-ed46-4d1c-b66b-ae092e53d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols = 3\n",
    "# for speaker_id in np.unique(df_speaker_waveform['speaker_id'])[:8]:\n",
    "#     fig, ax = plt.subplots(1, num_cols, figsize=(12, 3))\n",
    "#     waves = df_speaker_waveform[df_speaker_waveform['speaker_id']==speaker_id]['waveform'].values\n",
    "#     idx = df_speaker_waveform[df_speaker_waveform['speaker_id']==speaker_id]['waveform'].index\n",
    "#     for j, wave in enumerate(waves[:num_cols]):\n",
    "#         ax[j].specgram(wave, Fs=sample_rate)\n",
    "#         ax[j].set_title(f'speaker {speaker_id}, df.index: {idx[j]}')\n",
    "\n",
    "#         plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6eaa1-4cc4-41dc-a552-b05a9635e7eb",
   "metadata": {},
   "source": [
    "Visualize MFCC spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384cfced-73d2-4b13-9196-87266d078061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols = 3\n",
    "# for speaker_id in np.unique(df_speaker_waveform['speaker_id'])[:8]:\n",
    "#     fig, ax = plt.subplots(1, num_cols, figsize=(12, 3))\n",
    "#     waves = df_speaker_waveform[df_speaker_waveform['speaker_id']==speaker_id]['waveform'].values\n",
    "#     idx = df_speaker_waveform[df_speaker_waveform['speaker_id']==speaker_id]['waveform'].index\n",
    "#     for j, wave in enumerate(waves[:num_cols]):\n",
    "#         mfccs = librosa.feature.mfcc(y=wave.flatten(), n_mfcc=13, sr=sample_rate)\n",
    "#         img = librosa.display.specshow(mfccs, x_axis=\"time\", sr=sample_rate, ax=ax[j])\n",
    "#         ax[j].set_title(f'speaker: {speaker_id}, df.index: {idx[j]}')\n",
    "#         fig.colorbar(img, ax=ax[j], format=\"%+2.f dB\")\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f20275-84b6-443b-b502-49132582cd5f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85505199-318a-4da0-96fe-cb20aa02bb61",
   "metadata": {},
   "source": [
    "We decided to normalize our data by using the mean and variance of the amplitude. The normalizer is fitted on the whole training set, not for each speaker. The reasoning for this is that in our problem, we do not know ahead of time who the speaker is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdaf1a83-d532-4f75-8292-42b21b582c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_encoder(speaker_ids):\n",
    "    speaker_dict = {}\n",
    "    for i, speaker_id in enumerate(speaker_ids):\n",
    "        speaker_dict[speaker_id] = i\n",
    "        \n",
    "    return speaker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbd8797-35b1-4a7c-abc3-c5d54a977290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mfcc_databases(dataset, window_size : int, sample_rate : int, speaker_dict : dict, number_spectral_coefficients : int):\n",
    "    \"\"\"\n",
    "    Creates two databases from an audio dataset e.g. librispeech\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    speaker_mfcc_db : pd.DataFrame \n",
    "        df with shape (index_id, speaker_id, mfcc_id)\n",
    "    \n",
    "    mfcc_channel_db : pd.DataFrame\n",
    "        df with shape (mfcc_id, channel_id)\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        speaker_mfcc_db = pd.read_pickle(f'speaker_mfcc_db_{window_size}_{sample_rate}_{number_spectral_coefficients}.pkl')\n",
    "        mfcc_channel_db = pd.read_pickle(f'mfcc_channel_db_{window_size}_{sample_rate}_{number_spectral_coefficients}.pkl')\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        speaker_mfcc_db = pd.DataFrame(columns=[\"speaker_id\", \"mfcc_id\"])\n",
    "        mfcc_channel_db = pd.DataFrame(columns=[\"mfcc_id\"]+[f\"channel_{i}\" for i in range(number_spectral_coefficients)]).set_index('mfcc_id')\n",
    "\n",
    "        idx = 0\n",
    "\n",
    "        print('Creating a database. Hold tight...')\n",
    "        for i in tqdm(range(dataset.__len__())):\n",
    "            (waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id) = dataset.__getitem__(i)\n",
    "            waveform_arr = waveform.numpy().flatten()\n",
    "            splitted_waveform = split_waveform(waveform_arr, window_size) # split waveforms into consistent chunks\n",
    "\n",
    "            for chunk in splitted_waveform: # transform waveforms into MFCC spectrograms\n",
    "                mfccs = librosa.feature.mfcc(y=chunk.flatten(), n_mfcc=13, sr=sample_rate)\n",
    "\n",
    "                speaker_mfcc_db.loc[idx, \"speaker_id\"] = speaker_dict[speaker_id]\n",
    "                speaker_mfcc_db.loc[idx, \"mfcc_id\"] = idx\n",
    "\n",
    "                for j in range(number_spectral_coefficients):\n",
    "                    mfcc_channel_db.loc[idx, f\"channel_{j}\"] = 1\n",
    "                    mfcc_channel_db.at[idx, f\"channel_{j}\"] = mfccs[j, :]\n",
    "                \n",
    "                idx += 1\n",
    "\n",
    "        speaker_mfcc_db.to_pickle(f'speaker_mfcc_db_{window_size}_{sample_rate}_{number_spectral_coefficients}.pkl')\n",
    "        mfcc_channel_db.to_pickle(f'mfcc_channel_db_{window_size}_{sample_rate}_{number_spectral_coefficients}.pkl')\n",
    "\n",
    "    return speaker_mfcc_db, mfcc_channel_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a64272-d461-413c-81ac-ec7485fe353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(base_dataset, speaker_dict : dict, window_size : int, sample_rate : int, train_pct : float, \n",
    "                 val_pct : float, test_pct : float, number_spectral_coefficients : int, cepstral_normalize : bool, transform=None):\n",
    "    \"\"\"\n",
    "    Wrapper around other functions to create datasets\n",
    "    \n",
    "    Inputs\n",
    "    -----\n",
    "    \n",
    "    base_dataset : \n",
    "        A pytorch audio dataset like librispeech\n",
    "    \n",
    "    speaker_dict : dict\n",
    "        dictionary of {'speaker_id':encoded_id} pairs          \n",
    "\n",
    "    window_size : int\n",
    "        window size to be used to consistently split waveforms\n",
    "\n",
    "    sample_rate : int\n",
    "        sample rate in kHz\n",
    "\n",
    "    train_pct : float\n",
    "        train percentage\n",
    "    \n",
    "    val_pct : float\n",
    "        validation percentage\n",
    "\n",
    "    test_pct : float\n",
    "        Test percentage\n",
    "\n",
    "    number_spectral_coefficients : int\n",
    "        number of spectral coefficients to keep for the MFCC calculation\n",
    "        \n",
    "    normalize : bool\n",
    "        whether to apply cepstral normalization\n",
    "        \n",
    "    transform : \n",
    "        a custom transformation to apply to the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    speaker_mfcc_db, mfcc_channel_db = create_mfcc_databases(base_dataset, window_size=window_size, sample_rate=sample_rate, speaker_dict=speaker_dict, number_spectral_coefficients=number_spectral_coefficients)\n",
    "\n",
    "    train_ids, val_ids, test_ids = split_train_test_val(speaker_mfcc_db, train_pct, val_pct, test_pct)\n",
    "    \n",
    "    speaker_mfcc_db_train = speaker_mfcc_db.loc[train_ids]\n",
    "    speaker_mfcc_db_val = speaker_mfcc_db.loc[val_ids]\n",
    "    speaker_mfcc_db_test = speaker_mfcc_db.loc[test_ids]\n",
    "    \n",
    "    if cepstral_normalize:\n",
    "        cepstral_normalization = CepstralNormalization(number_spectral_coefficients)\n",
    "        cepstral_normalization.fit(speaker_mfcc_db_train['mfcc_id'].values, mfcc_channel_db)\n",
    "        mfcc_channel_db = cepstral_normalization.transform(speaker_mfcc_db_train['mfcc_id'].values, mfcc_channel_db)\n",
    "        mfcc_channel_db = cepstral_normalization.transform(speaker_mfcc_db_val['mfcc_id'].values, mfcc_channel_db)\n",
    "        mfcc_channel_db = cepstral_normalization.transform(speaker_mfcc_db_test['mfcc_id'].values, mfcc_channel_db)\n",
    "        \n",
    "    mfcc_channel_db_train = mfcc_channel_db.loc[speaker_mfcc_db_train['mfcc_id'].values]\n",
    "    mfcc_channel_db_val = mfcc_channel_db.loc[speaker_mfcc_db_val['mfcc_id'].values]\n",
    "    mfcc_channel_db_test = mfcc_channel_db.loc[speaker_mfcc_db_test['mfcc_id'].values]\n",
    "    \n",
    "    mfcc_dataset_train = MFCCData(speaker_mfcc_db_train, mfcc_channel_db_train, transform=transform)\n",
    "    mfcc_dataset_val = MFCCData(speaker_mfcc_db_val, mfcc_channel_db_val, transform=transform)\n",
    "    mfcc_dataset_test = MFCCData(speaker_mfcc_db_test, mfcc_channel_db_test, transform=transform)\n",
    "    \n",
    "    torch.save(mfcc_dataset_train, 'mfcc_dataset_train.pt')\n",
    "    torch.save(mfcc_dataset_val, 'mfcc_dataset_val.pt')\n",
    "    torch.save(mfcc_dataset_test, 'mfcc_dataset_test.pt')\n",
    "\n",
    "    return mfcc_dataset_train, mfcc_dataset_val, mfcc_dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3146596f-9d8c-457e-9e2e-13fe3df92d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_waveform(waveform : np.array, window_size : int) -> list[np.array]:\n",
    "    \"\"\"\n",
    "    Takes an original waveform and reduce to chunks of smaller intervals using a sliding window\n",
    "    \n",
    "    waveform : nd.array\n",
    "    \n",
    "    window_size : int\n",
    "        window size in seconds\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    splitted_waveform : list[nd.array]\n",
    "        list of waverforms\n",
    "    \n",
    "    \"\"\"\n",
    "    splitted_waveform = []\n",
    "    \n",
    "    for i in range(0, len(waveform), window_size):\n",
    "        split = waveform[i:i+window_size]\n",
    "        if len(split) == window_size:\n",
    "            splitted_waveform.append(split)\n",
    "\n",
    "    return splitted_waveform\n",
    "\n",
    "\n",
    "class CepstralNormalization:\n",
    "    def __init__(self, number_spectral_coefficients : int):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.number_spectral_coefficients = number_spectral_coefficients\n",
    "        self.mfcc_mean : list[float] | None = None\n",
    "        self.mfcc_std : list[float] | None = None\n",
    "    \n",
    "    def fit(self, mfcc_ids, mfcc_channel_db):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        mfcc_mean = []\n",
    "        mfcc_std = []\n",
    "        \n",
    "        print('Normalizing...')\n",
    "        for channel in tqdm(range(self.number_spectral_coefficients)):\n",
    "            channel_data = mfcc_channel_db.loc[mfcc_ids][f\"channel_{channel}\"].values\n",
    "            channel_data_list = []\n",
    "            for x in channel_data:\n",
    "                channel_data_list.extend(x.flatten().tolist())\n",
    "            \n",
    "            mfcc_mean.append(np.mean(channel_data_list))\n",
    "            mfcc_std.append(np.std(channel_data_list))\n",
    "\n",
    "        self.mfcc_mean, self.mfcc_std = mfcc_mean, mfcc_std\n",
    "        \n",
    "        return self.mfcc_mean, self.mfcc_std\n",
    "    \n",
    "    def transform(self, mfcc_ids, mfcc_channel_db):\n",
    "        \"\"\"\n",
    "        \"\"\"                \n",
    "        for channel in range(self.number_spectral_coefficients):\n",
    "            channel_data = mfcc_channel_db.loc[mfcc_ids][f\"channel_{channel}\"].values\n",
    "            channel_data = (channel_data - self.mfcc_mean[channel]) / self.mfcc_std[channel]\n",
    "            \n",
    "            mfcc_channel_db.loc[mfcc_ids][f\"channel_{channel}\"] = channel_data\n",
    "                \n",
    "        return mfcc_channel_db\n",
    "\n",
    "def array_to_image_transform(mfcc_arr):\n",
    "    mfcc_img = Image.fromarray(mfcc_arr, mode='L')\n",
    "    mfcc_img = np.array(mfcc_img)\n",
    "    return mfcc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17dc9537-d8e0-4e9f-930a-757f8841af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCData(Dataset):\n",
    "    def __init__(self, speaker_mfcc_db, mfcc_channel_db, transform=None):\n",
    "        \"\"\"\n",
    "        Creates a dataset of spectrograms from a dataset of waveforms\n",
    "        \n",
    "        train_test_val : str\n",
    "            one of 'train', 'validation', or 'test'\n",
    "        \"\"\"\n",
    "        self.speaker_mfcc_db = speaker_mfcc_db\n",
    "        self.mfcc_channel_db = mfcc_channel_db        \n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.speaker_mfcc_db)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speaker_id = self.speaker_mfcc_db.iloc[idx]['speaker_id']\n",
    "        mfcc_id = self.speaker_mfcc_db.iloc[idx]['mfcc_id']\n",
    "        \n",
    "        mfcc = []\n",
    "        for i, col in enumerate(self.mfcc_channel_db.columns):\n",
    "            mfcc_channel_i = self.mfcc_channel_db.loc[mfcc_id, f'channel_{i}']\n",
    "            mfcc.append(mfcc_channel_i)\n",
    "            \n",
    "        mfcc = np.asarray(mfcc)\n",
    "        mfcc = mfcc.reshape(len(self.mfcc_channel_db.columns), -1)\n",
    "        \n",
    "        if self.transform:\n",
    "            mfcc = array_to_image_transform(mfcc)\n",
    "            mfcc = self.transform(mfcc)\n",
    "            \n",
    "        speaker_id = torch.tensor(speaker_id) \n",
    "        mfcc = torch.tensor(mfcc) \n",
    "            \n",
    "        if torch.cuda.is_available():\n",
    "            speaker_id = speaker_id.to('cuda')\n",
    "            mfcc = mfcc.to('cuda')\n",
    "            \n",
    "        return mfcc, speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1935d22-d922-4834-a2d5-3b9c3c41ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(speaker_mfcc_db, train_pct, val_pct, test_pct):\n",
    "    \"\"\"\n",
    "    Samples from each speaker_id must be included in each: train, val, test\n",
    "    \n",
    "    \"\"\"\n",
    "    train_ids = []\n",
    "    val_ids = []\n",
    "    test_ids = []\n",
    "    \n",
    "    for speaker_id in np.unique(speaker_mfcc_db[\"speaker_id\"].values):\n",
    "        mfcc_ids = speaker_mfcc_db[speaker_mfcc_db[\"speaker_id\"]==speaker_id][\"mfcc_id\"].sample(frac=1).tolist()\n",
    "        len_ids = len(mfcc_ids)\n",
    "        \n",
    "        train_ids.extend(mfcc_ids[:int(len_ids * train_pct)])\n",
    "        val_ids.extend(mfcc_ids[int(len_ids * train_pct):int(len_ids * (train_pct + val_pct))])\n",
    "        test_ids.extend(mfcc_ids[int(len_ids * (train_pct + val_pct)):])\n",
    "    \n",
    "    return train_ids, val_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "402724bf-ba8a-49a4-b9fe-9369a0c0db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing parameters\n",
    "sample_rate = 16000 # 16kHz\n",
    "window_size = 4 * sample_rate # 1 second has 16000 samples, window_size is 4 seconds\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_pct = 0.70\n",
    "val_pct = 0.20\n",
    "test_pct = 0.10\n",
    "number_spectral_coefficients = 13\n",
    "cepstral_normalize = True\n",
    "\n",
    "custom_transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3bf8b-81aa-48fd-a117-65c414aa0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_dict = speaker_encoder(np.unique(df_spkr['speaker_id']))\n",
    "# mfcc_dataset_train, mfcc_dataset_val, mfcc_dataset_test = get_datasets(dataset, \n",
    "#                                                 speaker_dict=speaker_dict,\n",
    "#                                                 window_size=window_size, \n",
    "#                                                 sample_rate=sample_rate, \n",
    "#                                                 train_pct=train_pct,\n",
    "#                                                 val_pct=val_pct,\n",
    "#                                                 test_pct=test_pct,\n",
    "#                                                 number_spectral_coefficients=number_spectral_coefficients,\n",
    "#                                                 cepstral_normalize=cepstral_normalize,\n",
    "#                                                 transform=custom_transform,\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a109eee-793b-4d39-998e-51dd568abb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_dataset_train = torch.load('mfcc_dataset_train.pt')\n",
    "mfcc_dataset_val = torch.load('mfcc_dataset_val.pt')\n",
    "mfcc_dataset_test = torch.load('mfcc_dataset_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e949d-a113-41a5-a534-5c0667068f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a550cd0-2ec1-4f8b-82f4-5b726ac174d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mfcc_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(mfcc_dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mfcc_dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe68cbb3-45eb-4a43-9ffb-7ff343d5a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52097, 14909, 7584)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_dataset_train.__len__(), mfcc_dataset_val.__len__(), mfcc_dataset_test.__len__(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34fccb02-5bf8-47db-868f-95a73feb0c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12883/3454332804.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mfcc = torch.tensor(mfcc)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13, 126]), torch.Size([]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_mfcc, sample_speaker = mfcc_dataset_train.__getitem__(0)\n",
    "sample_mfcc.shape, sample_speaker.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58937ba-1840-4675-a419-46fb4cd2bcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f510f6ac970>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.transpose(sample_mfcc.to('cpu').numpy(), [1, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955efff-b90a-40d9-9187-9f7577752e1c",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c9bd128-f7db-4644-b0bc-ea4d8d735a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\" Evaluate the network on the validation set.\n",
    "\n",
    "     Args:\n",
    "         net: PyTorch neural network object\n",
    "         loader: PyTorch data loader for the validation set\n",
    "         criterion: The loss function\n",
    "     Returns:\n",
    "         err: A scalar for the avg classification error over the validation set\n",
    "         loss: A scalar for the average loss function over the validation set\n",
    "     \"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_epoch = 0\n",
    "    val_acc = []\n",
    "\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        mfcc, speakers = data\n",
    "        outputs = model(mfcc)\n",
    "        \n",
    "        loss = criterion(outputs, speakers)\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(speakers)\n",
    "        val_acc.append(get_accuracy(output=outputs, mfcc=mfcc, speakers=speakers))\n",
    "        \n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    val_acc = np.mean(val_acc)\n",
    "    return loss, val_acc\n",
    "\n",
    "def get_accuracy(output, mfcc, speakers):\n",
    "    #select index with maximum prediction score\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(speakers.view_as(pred)).sum().item()\n",
    "    total += mfcc.shape[0]\n",
    "    return correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, batch_size, num_epochs, criterion, optimizer, print_every=2, plot_loss=True):\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    train_acc = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(torch.device('cuda:0'))\n",
    "    \n",
    "    for epoch in range(num_epochs): \n",
    "        total_train_loss = 0.0\n",
    "        total_epoch = 0\n",
    "        total_train_acc = []\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "            mfcc, speakers = data\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass, backward pass, and optimize\n",
    "            outputs = model(mfcc)\n",
    "            loss = criterion(outputs, speakers)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate the statistics\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(speakers)\n",
    "            total_train_acc.append(get_accuracy(output=outputs, mfcc=mfcc, speakers=speakers))\n",
    "            \n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        train_acc[epoch] = np.mean(total_train_acc)        \n",
    "        val_loss[epoch], val_acc[epoch] = evaluate(model, val_loader, criterion)\n",
    "        \n",
    "        if epoch % print_every == 0:    \n",
    "            print(f\"Epoch {epoch}: \\n \\\n",
    "            Train loss {train_loss[epoch]:.3f} | Val loss {val_loss[epoch]:.3f} \\n \\\n",
    "            Train acc {train_acc[epoch]:.3f} | Val acc {val_acc[epoch]:.3f} \\n\")\n",
    "            \n",
    "            \n",
    "    if plot_loss:\n",
    "        n = len(train_loss) # number of epochs\n",
    "        plt.figure()\n",
    "        plt.title(\"Train vs Validation Loss\")\n",
    "        plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "        plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend(loc='best')\n",
    "        # plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title(\"Train vs Validation Accuracy\")\n",
    "        plt.plot(range(1,n+1), train_acc, label=\"Train\")\n",
    "        plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend(loc='best')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "735bfed3-71b5-444b-94e6-8cb2f8c68be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_layers, hidden_layers, output_size):\n",
    "        super(FCN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(3 * 11 * 124, hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = x.view(-1, 13 * 126)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2955d4c-317d-4f04-a91e-653456210aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_layers, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(1 * 13 * 126, hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 1 * 13 * 126)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "461aa166-0a56-4ef4-b121-36e3d14812b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "hidden_layers = 100\n",
    "output_size = 40\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "num_epochs = 30\n",
    "\n",
    "model = CNN(hidden_layers=hidden_layers, output_size=output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "num_epochs = num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d3cdaee-66f7-46fe-81e9-bc6a3c8d87c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(model, train_loader, val_loader, batch_size, num_epochs, criterion, optimizer, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn [16], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, batch_size, num_epochs, criterion, optimizer, print_every, plot_loss)\u001b[0m\n\u001b[1;32m     42\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(num_epochs)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 45\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs): \n\u001b[1;32m     48\u001b[0m     total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl-env/lib/python3.10/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, batch_size, num_epochs, criterion, optimizer, print_every=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1b155-3e22-4653-a275-d448e73e0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            mfcc, speakers = data\n",
    "            outputs = model(mfcc)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += speakers.size(0)\n",
    "            correct += (predicted == speakers).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac5c23-631a-446b-aadb-33205fe653bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf007c7-2f74-48cd-af33-c4d9eeaf67c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mfcc, speakers in test_loader:\n",
    "    outputs = model(mfcc)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(f\"predicted: {predicted}, actual: {speakers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2eb77-8e8a-43cd-803e-7720e7d0a2df",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c848ac6-a207-49da-9f3a-b1c555b4191c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.features[0] = nn.Conv2d(1, 64, kernel_size=(11,11), stride=(4,4), padding=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca0fa3-0aac-4824-88aa-8128c5d5d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0118b-aeda-4b33-9587-f27fcae31d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the final output layer\n",
    "\n",
    "alexnet.classifier[6] = nn.Linear(4096, 500)\n",
    "alexnet.classifier[7] = nn.LeakyReLU()\n",
    "alexnet.classifier[8] = nn.Linear(500, 40)\n",
    "alexnet.classifier.add_module(\"9\", nn.LogSoftmax(dim = 1))\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0dfff-9061-4636-90df-0c04c8bb2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(alexnet.parameters())\n",
    "\n",
    "\n",
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            \n",
    "            # Backpropagate the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            \n",
    "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "            \n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(val_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "            \n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/2399 \n",
    "        avg_train_acc = train_acc/2399\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/686 \n",
    "        avg_valid_acc = valid_acc/686\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "                \n",
    "        epoch_end = time.time()\n",
    "    \n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "        \n",
    "        # Save if the model has best accuracy till now\n",
    "        #torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
    "            \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73dc03-bf89-49eb-a066-198be5a06cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5df8a-9e0b-4bc8-b2ed-1f7f9504ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = alexnet.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127a545-4726-41ab-9b0e-c700842cadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(alexnet, loss_func, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34566aa-8b75-4042-aff7-903df75b97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "ep = []\n",
    "i = 0\n",
    "for tacc, vacc, tloss, vloss in history:\n",
    "    train_acc.append(tacc)\n",
    "    val_acc.append(vacc)\n",
    "    train_loss.append(tloss)\n",
    "    val_loss.append(vloss)\n",
    "    ep.append(i)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b468ff5-7f0d-423e-a993-768e18bbb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train_loss) # number of epochs\n",
    "plt.figure()\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "# plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Train vs Validation Accuracy\")\n",
    "plt.plot(range(1,n+1), train_acc, label=\"Train\")\n",
    "plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de85610-13b3-4e2b-ac33-0ff43aceb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mfcc, speakers in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06957a55-4718-45e3-8068-3ae10a213ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(mfcc[0].to('cpu').numpy(), [1, 2, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
